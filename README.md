# V-JEPA 2 Implementation

å®Œå…¨ãª**V-JEPA 2 (Video Joint Embedding Predictive Architecture 2)** ã®å®Ÿè£…ã§ã™ã€‚Meta AIã®è«–æ–‡ã«å¿ å®Ÿã«å†ç¾ã—ã¦ã„ã¾ã™ã€‚

## ğŸ“„ è«–æ–‡æƒ…å ±

- **ã‚¿ã‚¤ãƒˆãƒ«**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning
- **è‘—è€…**: Mido Assran et al. (Meta AI)
- **arXiv**: [2506.09985](https://arxiv.org/abs/2506.09985)
- **å…¬é–‹æ—¥**: 2025å¹´6æœˆ
- **å…¬å¼å®Ÿè£…**: [github.com/facebookresearch/vjepa2](https://github.com/facebookresearch/vjepa2)

## ğŸ¯ æ¦‚è¦

V-JEPA 2ã¯ã€å¤§è¦æ¨¡å‹•ç”»ãƒ‡ãƒ¼ã‚¿ï¼ˆ100ä¸‡æ™‚é–“è¶…ï¼‰ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

### ä¸»ãªç‰¹å¾´

- **å‹•ä½œç†è§£**: Something-Something v2ã§77.3%ã®top-1ç²¾åº¦
- **è¡Œå‹•äºˆæ¸¬**: Epic-Kitchens-100ã§39.7%ã®recall-at-5
- **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹å¿œç”¨**: ã‚ãšã‹62æ™‚é–“ã®æœªãƒ©ãƒ™ãƒ«ãƒ­ãƒœãƒƒãƒˆå‹•ç”»ã§è¡Œå‹•æ¡ä»¶ä»˜ãä¸–ç•Œãƒ¢ãƒ‡ãƒ«ï¼ˆV-JEPA 2-ACï¼‰ã‚’æ§‹ç¯‰
- **ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè¨ˆç”»**: ãƒ•ãƒ©ãƒ³ã‚«ã‚¢ãƒ¼ãƒ ã§ã®ãƒ”ãƒƒã‚¯&ãƒ—ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ã‚’ç”»åƒã‚´ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œ

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1. äº‹å‰å­¦ç¿’ï¼ˆV-JEPA 2ï¼‰

```
å…¥åŠ›å‹•ç”» â†’ [Context Encoder (ViT)] â†’ å¯è¦–ãƒˆãƒ¼ã‚¯ãƒ³
                    â†“
         [Predictor] â†’ ãƒã‚¹ã‚¯é ˜åŸŸã®äºˆæ¸¬
                    â†“
         [Target Encoder (EMA)] â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡¨ç¾
                    â†“
              L1 Lossï¼ˆäºˆæ¸¬ vs ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
```

**ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**:
- **Context Encoder**: Vision Transformerï¼ˆViT-L/H/gã€300M-1B parametersï¼‰
- **Target Encoder**: Context Encoderã®EMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ã‚³ãƒ”ãƒ¼
- **Predictor**: å°è¦æ¨¡ViTã€ãƒã‚¹ã‚¯é ˜åŸŸã®è¡¨ç¾ã‚’äºˆæ¸¬
- **æå¤±é–¢æ•°**: L1ãƒãƒ«ãƒ ï¼ˆäºˆæ¸¬è¡¨ç¾ vs ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡¨ç¾ï¼‰

### 2. è¡Œå‹•æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ï¼ˆV-JEPA 2-ACï¼‰

```
ç¾åœ¨ã®æ½œåœ¨çŠ¶æ…‹ z_t + è¡Œå‹• a_t â†’ [Action-Conditioned Predictor] â†’ æ¬¡çŠ¶æ…‹ z_{t+1}
```

**ç”¨é€”**: ãƒ­ãƒœãƒƒãƒˆæ“ä½œã€å‹•ç”»äºˆæ¸¬ã€è¨ˆç”»

## ğŸ“¦ å®Ÿè£…å†…å®¹

### ã‚³ã‚¢å®Ÿè£…

```
vjepa2/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vision_transformer.py     # ViTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å®Ÿè£…
â”‚   â”œâ”€â”€ predictor.py               # äºˆæ¸¬å™¨ï¼ˆæ¨™æº–ãƒ»è¡Œå‹•æ¡ä»¶ä»˜ãï¼‰
â”‚   â””â”€â”€ vjepa2.py                  # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ position_encoding.py      # 3Dä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆsinusoidal + RoPEï¼‰
â”‚   â””â”€â”€ masking.py                 # ãƒãƒ«ãƒãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚­ãƒ³ã‚°
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py                 # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
â”‚   â””â”€â”€ losses.py                  # æå¤±é–¢æ•°
â””â”€â”€ data/
    â””â”€â”€ video_dataset.py           # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
```

### Google Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

**`vjepa2_imagenet_finetuning.ipynb`**: ImageNetã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

- âœ… V-JEPA 2ã®å®Œå…¨å®Ÿè£…ï¼ˆ1ã¤ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼‰
- âœ… äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
- âœ… Linear Probing / Full Fine-tuning
- âœ… å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—
- âœ… çµæœã®å¯è¦–åŒ–

## ğŸš€ ä½¿ã„æ–¹

### 1. Google Colabã§å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰

1. `vjepa2_imagenet_finetuning.ipynb`ã‚’Google Colabã§é–‹ã
2. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’GPUã«è¨­å®š
3. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ

```python
# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# 2. V-JEPA 2ãƒ¢ãƒ‡ãƒ«å®Ÿè£…
# 3. äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
# 4. ImageNetãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
# 5. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
# 6. çµæœã®å¯è¦–åŒ–
```

### 2. ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œ

```bash
# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision timm einops transformers

# Jupyter Notebookã‚’èµ·å‹•
jupyter notebook vjepa2_imagenet_finetuning.ipynb
```

### 3. äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•

V-JEPA 2ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®æ–¹æ³•ã§ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š

#### æ–¹æ³•1: PyTorch Hubï¼ˆæ¨å¥¨ï¼‰

```python
import torch

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
model = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_large', pretrained=True)  # ViT-L (300M)
model = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_huge', pretrained=True)   # ViT-H (600M)
model = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_giant', pretrained=True)  # ViT-g (1B)
```

#### æ–¹æ³•2: Hugging Face Hub

```python
from transformers import AutoModel

model = AutoModel.from_pretrained('facebook/vjepa2-vit-large', trust_remote_code=True)
```

**æ³¨æ„**: ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯è‡ªå‹•çš„ã«ã“ã‚Œã‚‰ã®æ–¹æ³•ã‚’è©¦è¡Œã—ã€åˆ©ç”¨å¯èƒ½ãªæ–¹æ³•ã§é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

## ğŸ“Š ä¸»è¦ãªå®Ÿè£…è©³ç´°

### 1. 3Dä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```python
# 3D sinusoidal position embeddings for video (T x H x W)
pos_embed = get_3d_sincos_pos_embed(
    embed_dim=1024,
    grid_size=14,      # ç©ºé–“ã‚°ãƒªãƒƒãƒ‰ (224/16)
    grid_depth=8,      # æ™‚é–“ã‚°ãƒªãƒƒãƒ‰ (16/2)
    cls_token=True
)
```

**ç‰¹å¾´**:
- æ™‚é–“ãƒ»é«˜ã•ãƒ»å¹…ã®3æ¬¡å…ƒã«åˆ†å‰²
- å›ºå®šsinusoidalåŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’ä¸è¦ï¼‰
- 3D RoPEã«ã‚‚å¯¾å¿œ

### 2. ãƒãƒ«ãƒãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚­ãƒ³ã‚°

```python
# Spatiotemporal block masking
mask_generator = MultiBlockMaskGenerator(
    input_size=(8, 14, 14),  # (T, H, W) in patches
    num_masks=4,              # ãƒã‚¹ã‚¯ãƒ–ãƒ­ãƒƒã‚¯æ•°
    min_area=0.15,            # æœ€å°é¢ç©æ¯”
    max_area=0.7              # æœ€å¤§é¢ç©æ¯”
)

encoder_mask, predictor_masks = mask_generator()
```

**ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥**:
- ãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚ç©ºé–“ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒã‚¹ã‚¯
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å¯è¦–ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‡¦ç†
- äºˆæ¸¬å™¨ã¯ãƒã‚¹ã‚¯é ˜åŸŸã‚’äºˆæ¸¬

### 3. EMAã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

```python
# Exponential Moving Average update
@torch.no_grad()
def update_target_encoder(momentum=0.996):
    for param_q, param_k in zip(encoder.parameters(),
                                 target_encoder.parameters()):
        param_k.data.mul_(momentum).add_(param_q.data, alpha=1-momentum)
```

**æ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:
- åˆæœŸmomentum: 0.996
- ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆè«–æ–‡ã«å¾“ã†ï¼‰
- å‹¾é…ã‚’ä¼æ’­ã•ã›ãªã„ï¼ˆstop-gradientï¼‰

### 4. æå¤±é–¢æ•°

```python
# L1 regression loss
def vjepa_loss(predictions, targets, loss_exp=1.0):
    loss = 0
    for pred, target in zip(predictions, targets):
        loss += torch.mean(torch.abs(pred - target) ** loss_exp) / loss_exp
    return loss / len(predictions)
```

**æå¤±ã®ç‰¹å¾´**:
- L1ãƒãƒ«ãƒ ï¼ˆè«–æ–‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- Lpãƒãƒ«ãƒ ã«ä¸€èˆ¬åŒ–å¯èƒ½
- ãƒã‚¹ã‚¯é ˜åŸŸã”ã¨ã«è¨ˆç®—

## ğŸ“ ImageNetãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### Linear Probing

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å‡çµã—ã€ç·šå½¢åˆ†é¡å™¨ã®ã¿ã‚’å­¦ç¿’ï¼š

```python
model = ImageNetClassifier(
    encoder=pretrained_encoder,
    num_classes=1000,
    freeze_encoder=True  # Linear Probing
)

# é«˜ã„å­¦ç¿’ç‡
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
```

### Full Fine-tuning

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å…¨ä½“ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š

```python
model = ImageNetClassifier(
    encoder=pretrained_encoder,
    num_classes=1000,
    freeze_encoder=False  # Full Fine-tuning
)

# ä½ã„å­¦ç¿’ç‡
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)
```

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½

### ImageNet-1K

| ãƒ¢ãƒ‡ãƒ« | Top-1 Acc | Top-5 Acc | Parameters |
|--------|-----------|-----------|------------|
| ViT-L  | ~85%      | ~97%      | 300M       |
| ViT-H  | ~86%      | ~98%      | 600M       |
| ViT-g  | ~87%      | ~98%      | 1B         |

*æ³¨: å®Ÿéš›ã®æ€§èƒ½ã¯å­¦ç¿’è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿é‡ã«ä¾å­˜*

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¤‰æ›´

```python
# ViT-Large (300M)
encoder = build_vjepa2_encoder('vitl', num_frames=16)

# ViT-Huge (600M)
encoder = build_vjepa2_encoder('vith', num_frames=16)

# ViT-giant (1B)
encoder = build_vjepa2_encoder('vitg', num_frames=16)
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

```python
# å­¦ç¿’è¨­å®š
EPOCHS = 100
LEARNING_RATE = 0.001  # Linear probing
BATCH_SIZE = 256
WARMUP_EPOCHS = 10
```

## ğŸ¤– ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹å¿œç”¨ï¼ˆV-JEPA 2-ACï¼‰

è¡Œå‹•æ¡ä»¶ä»˜ãäºˆæ¸¬å™¨ã‚’ä½¿ç”¨ã—ãŸä¸–ç•Œãƒ¢ãƒ‡ãƒ«ï¼š

```python
# Build action-conditioned model
model_ac = VJEPA2_AC(
    vjepa2_encoder=pretrained_encoder,
    action_dim=7,  # ãƒ­ãƒœãƒƒãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¬¡å…ƒ
    freeze_encoder=True
)

# Predict next state
z_current = model_ac.encode(observation)
z_next = model_ac(z_current, action)

# Plan actions to reach goal
actions = model_ac.plan(
    z_init=z_current,
    z_goal=z_goal,
    horizon=10
)
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **V-JEPA 2 Paper**: [arXiv:2506.09985](https://arxiv.org/abs/2506.09985)
2. **å…¬å¼å®Ÿè£…**: [github.com/facebookresearch/vjepa2](https://github.com/facebookresearch/vjepa2)
3. **Blog**: [ai.meta.com/vjepa](https://ai.meta.com/vjepa/)
4. **åŸè«–æ–‡ I-JEPA**: [arXiv:2301.08243](https://arxiv.org/abs/2301.08243)

## ğŸ™ è¬è¾

ã“ã®å®Ÿè£…ã¯ã€Meta AIã®V-JEPA 2è«–æ–‡ã¨å…¬å¼å®Ÿè£…ã‚’åŸºã«ã—ã¦ã„ã¾ã™ã€‚

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ•™è‚²ç›®çš„ã§ä½œæˆã•ã‚Œã¾ã—ãŸã€‚å•†ç”¨åˆ©ç”¨ã™ã‚‹å ´åˆã¯ã€Meta AIã®å…¬å¼ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

---

**å®Ÿè£…è€…**: V-JEPA 2ã®è«–æ–‡ã«å¿ å®Ÿãªå®Œå…¨å®Ÿè£…
**æœ€çµ‚æ›´æ–°**: 2025å¹´11æœˆ
