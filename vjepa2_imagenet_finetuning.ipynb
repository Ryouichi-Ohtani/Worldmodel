{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA 2: ImageNet Fine-tuning with Pretrained Weights\n",
    "\n",
    "このノートブックは、V-JEPA 2の事前学習済みモデルを使用してImageNetでファインチューニングを行います。\n",
    "\n",
    "## 論文情報\n",
    "- **Title**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning\n",
    "- **arXiv**: 2506.09985\n",
    "- **Authors**: Mido Assran et al. (Meta AI)\n",
    "- **Published**: June 2025\n",
    "\n",
    "## 実装内容\n",
    "1. V-JEPA 2モデルの完全実装\n",
    "2. 事前学習済み重みのロード\n",
    "3. ImageNetデータセットの準備\n",
    "4. Linear Probing / Full Fine-tuning\n",
    "5. 学習と評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップと依存関係のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境の確認\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# GPU確認\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install -q timm einops\n",
    "!pip install -q torchvision\n",
    "!pip install -q tqdm\n",
    "!pip install -q wandb  # オプション: ログ記録用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. V-JEPA 2モデルの実装\n",
    "\n",
    "論文に忠実な完全実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Position Encoding Utilities\n",
    "# ============================================================================\n",
    "\n",
    "def get_3d_sincos_pos_embed(\n",
    "    embed_dim: int,\n",
    "    grid_size: int,\n",
    "    grid_depth: int,\n",
    "    cls_token: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    3D sinusoidal position embeddings for video (T x H x W).\n",
    "    \n",
    "    Note: Flexibly handles embed_dim not divisible by 3.\n",
    "    Dimensions are partitioned ensuring all are even (required for sinusoidal encoding).\n",
    "    \"\"\"\n",
    "    # Flexible dimension partitioning (removed assertion for compatibility with standard ViT configs)\n",
    "    # Ensure all dimensions are even (required for sinusoidal encoding)\n",
    "    dim_t = (embed_dim // 3) // 2 * 2  # Round down to nearest even number\n",
    "    dim_h = ((embed_dim - dim_t) // 2) // 2 * 2  # Round down to nearest even number\n",
    "    dim_w = embed_dim - dim_t - dim_h  # Remainder\n",
    "\n",
    "    # Generate 3D grid\n",
    "    grid_t = np.arange(grid_depth, dtype=np.float32)\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "\n",
    "    grid = np.meshgrid(grid_t, grid_h, grid_w, indexing='ij')\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([3, -1]).T\n",
    "\n",
    "    # Generate embeddings\n",
    "    pos_embed_t = get_1d_sincos_pos_embed_from_grid(dim_t, grid[:, 0])\n",
    "    pos_embed_h = get_1d_sincos_pos_embed_from_grid(dim_h, grid[:, 1])\n",
    "    pos_embed_w = get_1d_sincos_pos_embed_from_grid(dim_w, grid[:, 2])\n",
    "\n",
    "    pos_embed = np.concatenate([pos_embed_t, pos_embed_h, pos_embed_w], axis=1)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return torch.from_numpy(pos_embed).float()\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1D sinusoidal position embeddings.\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "\n",
    "    pos = pos.reshape(-1)\n",
    "    out = np.einsum('m,d->md', pos, omega)\n",
    "\n",
    "    emb_sin = np.sin(out)\n",
    "    emb_cos = np.cos(out)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Patch Embedding\n",
    "# ============================================================================\n",
    "\n",
    "class PatchEmbed3D(nn.Module):\n",
    "    \"\"\"3D patch embedding for video.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        tubelet_size: int = 2,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.tubelet_size = tubelet_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=(tubelet_size, patch_size, patch_size),\n",
    "            stride=(tubelet_size, patch_size, patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int, int]]:\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = self.proj(x)\n",
    "\n",
    "        T_patches = x.shape[2]\n",
    "        H_patches = x.shape[3]\n",
    "        W_patches = x.shape[4]\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x, (T_patches, H_patches, W_patches)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Transformer Components\n",
    "# ============================================================================\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Efficient attention using SDPA if available\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.0\n",
    "            )\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Vision Transformer\n",
    "# ============================================================================\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer for V-JEPA 2.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        tubelet_size: int = 2,\n",
    "        in_channels: int = 3,\n",
    "        num_frames: int = 16,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        use_cls_token: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.tubelet_size = tubelet_size\n",
    "        self.num_frames = num_frames\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_cls_token = use_cls_token\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbed3D(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            tubelet_size=tubelet_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        grid_size = img_size // patch_size\n",
    "        grid_depth = num_frames // tubelet_size\n",
    "        num_patches = grid_depth * grid_size * grid_size\n",
    "\n",
    "        # CLS token\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "            num_patches += 1\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        # Position embedding\n",
    "        pos_embed = get_3d_sincos_pos_embed(\n",
    "            embed_dim,\n",
    "            grid_size,\n",
    "            grid_depth,\n",
    "            cls_token=use_cls_token\n",
    "        )\n",
    "        self.register_buffer('pos_embed', pos_embed.unsqueeze(0))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        self.apply(self._init_layer_weights)\n",
    "\n",
    "    def _init_layer_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_all_tokens: bool = False) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch embedding\n",
    "        x, _ = self.patch_embed(x)\n",
    "\n",
    "        # Add CLS token\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_all_tokens or self.cls_token is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x[:, 0]\n",
    "\n",
    "\n",
    "print(\"✓ V-JEPA 2 model components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. V-JEPA 2モデルのビルド\n",
    "\n",
    "ViT-L/g構成でモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vjepa2_encoder(model_size='vitl', num_frames=16):\n",
    "    \"\"\"\n",
    "    Build V-JEPA 2 encoder.\n",
    "\n",
    "    Args:\n",
    "        model_size: 'vitl' (300M), 'vith' (600M), or 'vitg' (1B)\n",
    "        num_frames: Number of input frames\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'vitl': {\n",
    "            'embed_dim': 1024,\n",
    "            'depth': 24,\n",
    "            'num_heads': 16,\n",
    "        },\n",
    "        'vith': {\n",
    "            'embed_dim': 1280,\n",
    "            'depth': 32,\n",
    "            'num_heads': 16,\n",
    "        },\n",
    "        'vitg': {\n",
    "            'embed_dim': 1408,\n",
    "            'depth': 40,\n",
    "            'num_heads': 16,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if model_size not in configs:\n",
    "        raise ValueError(f\"Unknown model size: {model_size}\")\n",
    "\n",
    "    config = configs[model_size]\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        tubelet_size=2,\n",
    "        in_channels=3,\n",
    "        num_frames=num_frames,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=4.0,\n",
    "        use_cls_token=True  # ImageNet fine-tuningではCLSトークンを使用\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# モデルサイズを選択（'vitl', 'vith', 'vitg'）\n",
    "MODEL_SIZE = 'vitl'  # ViT-Large (300M parameters)\n",
    "NUM_FRAMES = 16\n",
    "\n",
    "encoder = build_vjepa2_encoder(MODEL_SIZE, NUM_FRAMES)\n",
    "\n",
    "# パラメータ数を計算\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: V-JEPA 2 {MODEL_SIZE.upper()}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 事前学習済み重みのダウンロードとロード\n",
    "\n",
    "Meta AIの公式リポジトリから事前学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_pretrained_weights(model_size='vitl'):\n",
    "    \"\"\"\n",
    "    Download pretrained V-JEPA 2 weights.\n",
    "\n",
    "    Note: これは仮のURLです。実際のURLはMeta AIのリポジトリを確認してください。\n",
    "    https://github.com/facebookresearch/vjepa2\n",
    "    \"\"\"\n",
    "    # 事前学習済みモデルのURL（実際のURLに置き換えてください）\n",
    "    model_urls = {\n",
    "        'vitl': 'https://dl.fbaipublicfiles.com/vjepa2/vitl_pretrained.pth',\n",
    "        'vith': 'https://dl.fbaipublicfiles.com/vjepa2/vith_pretrained.pth',\n",
    "        'vitg': 'https://dl.fbaipublicfiles.com/vjepa2/vitg_pretrained.pth',\n",
    "    }\n",
    "\n",
    "    if model_size not in model_urls:\n",
    "        raise ValueError(f\"Unknown model size: {model_size}\")\n",
    "\n",
    "    url = model_urls[model_size]\n",
    "    save_path = f'vjepa2_{model_size}_pretrained.pth'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"✓ Pretrained weights already exist: {save_path}\")\n",
    "        return save_path\n",
    "\n",
    "    print(f\"Downloading pretrained weights for {model_size}...\")\n",
    "    try:\n",
    "        # torch.hubを使用してダウンロード（より確実）\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url,\n",
    "            map_location='cpu',\n",
    "            progress=True\n",
    "        )\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"✓ Downloaded to {save_path}\")\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading: {e}\")\n",
    "        print(\"Please download manually from: https://github.com/facebookresearch/vjepa2\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 代替方法: torch.hubから直接ロード\n",
    "def load_pretrained_from_hub(model_size='vitl'):\n",
    "    \"\"\"\n",
    "    Load pretrained model directly from torch.hub.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading V-JEPA 2 {model_size} from torch.hub...\")\n",
    "        # 実際のコード例（URLは仮）\n",
    "        encoder = torch.hub.load(\n",
    "            'facebookresearch/vjepa2',\n",
    "            f'vjepa2_{model_size}',\n",
    "            pretrained=True\n",
    "        )\n",
    "        print(\"✓ Loaded successfully\")\n",
    "        return encoder\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ torch.hub loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 重みをロード\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Loading Pretrained Weights\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 方法1: torch.hubから直接ロード（推奨）\n",
    "pretrained_encoder = load_pretrained_from_hub(MODEL_SIZE)\n",
    "\n",
    "if pretrained_encoder is None:\n",
    "    # 方法2: 手動ダウンロード\n",
    "    print(\"\\nTrying manual download...\")\n",
    "    weights_path = download_pretrained_weights(MODEL_SIZE)\n",
    "\n",
    "    if weights_path and os.path.exists(weights_path):\n",
    "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "\n",
    "        # チェックポイントの構造を確認\n",
    "        if 'encoder' in checkpoint:\n",
    "            state_dict = checkpoint['encoder']\n",
    "        elif 'model' in checkpoint:\n",
    "            state_dict = checkpoint['model']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "\n",
    "        # 重みをロード\n",
    "        encoder.load_state_dict(state_dict, strict=False)\n",
    "        print(\"✓ Loaded pretrained weights\")\n",
    "        pretrained_encoder = encoder\n",
    "    else:\n",
    "        print(\"\\n⚠ Could not load pretrained weights\")\n",
    "        print(\"Proceeding with random initialization for demonstration\")\n",
    "        pretrained_encoder = encoder\n",
    "else:\n",
    "    encoder = pretrained_encoder\n",
    "\n",
    "print(\"\\n✓ Encoder ready for fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ImageNet分類ヘッドの追加\n",
    "\n",
    "事前学習済みエンコーダに分類ヘッドを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    V-JEPA 2 encoder + linear classification head for ImageNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        num_classes: int = 1000,\n",
    "        freeze_encoder: bool = True,\n",
    "        use_video_frames: bool = False,\n",
    "        num_frames: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder: Pretrained V-JEPA 2 encoder\n",
    "            num_classes: Number of ImageNet classes\n",
    "            freeze_encoder: Whether to freeze encoder (linear probing)\n",
    "            use_video_frames: Whether to use multiple frames or single image\n",
    "            num_frames: Number of frames if use_video_frames=True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.use_video_frames = use_video_frames\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Freeze encoder for linear probing\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"✓ Encoder frozen (Linear Probing mode)\")\n",
    "        else:\n",
    "            print(\"✓ Encoder unfrozen (Full Fine-tuning mode)\")\n",
    "\n",
    "        # Classification head\n",
    "        embed_dim = encoder.embed_dim\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize head\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input images (B, C, H, W) or videos (B, C, T, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Logits (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Convert single image to video format if needed\n",
    "        if x.dim() == 4:  # (B, C, H, W)\n",
    "            # Repeat frames to match expected video input\n",
    "            x = x.unsqueeze(2).repeat(1, 1, self.num_frames, 1, 1)\n",
    "\n",
    "        # Encode\n",
    "        features = self.encoder(x, return_all_tokens=False)  # (B, embed_dim)\n",
    "\n",
    "        # Classify\n",
    "        logits = self.head(features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# モデルを構築\n",
    "NUM_CLASSES = 1000  # ImageNet-1K\n",
    "FREEZE_ENCODER = True  # True: Linear Probing, False: Full Fine-tuning\n",
    "\n",
    "model = ImageNetClassifier(\n",
    "    encoder=pretrained_encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_encoder=FREEZE_ENCODER,\n",
    "    num_frames=NUM_FRAMES\n",
    ")\n",
    "\n",
    "# GPUに移動\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# パラメータ数を確認\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ImageNetデータセットの準備\n",
    "\n",
    "ImageNetデータセットをロードして前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNetデータセットのパス設定\n",
    "# Google Driveにマウントする場合\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IMAGENET_PATH = '/content/drive/MyDrive/imagenet'  # 適宜変更\n",
    "else:\n",
    "    IMAGENET_PATH = './imagenet'\n",
    "\n",
    "# データ拡張とデータローダー\n",
    "def get_transforms(is_training=True):\n",
    "    \"\"\"\n",
    "    Get ImageNet transforms.\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "# データローダー設定\n",
    "BATCH_SIZE = 256  # GPUメモリに応じて調整\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "try:\n",
    "    # ImageNetデータセットをロード\n",
    "    train_dataset = ImageFolder(\n",
    "        os.path.join(IMAGENET_PATH, 'train'),\n",
    "        transform=get_transforms(is_training=True)\n",
    "    )\n",
    "    val_dataset = ImageFolder(\n",
    "        os.path.join(IMAGENET_PATH, 'val'),\n",
    "        transform=get_transforms(is_training=False)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"✓ ImageNet dataset loaded\")\n",
    "    print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "    print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading ImageNet: {e}\")\n",
    "    print(\"\\nUsing CIFAR-10 for demonstration...\")\n",
    "\n",
    "    # デモ用にCIFAR-10を使用\n",
    "    from torchvision.datasets import CIFAR10\n",
    "\n",
    "    train_dataset = CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=get_transforms(is_training=True)\n",
    "    )\n",
    "    val_dataset = CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=get_transforms(is_training=False)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # モデルを10クラスに変更\n",
    "    model.head = nn.Linear(model.encoder.embed_dim, 10).to(device)\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    print(f\"✓ CIFAR-10 dataset loaded (demo)\")\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001 if FREEZE_ENCODER else 0.0001  # Linear probingは高めのLR\n",
    "WEIGHT_DECAY = 0.0\n",
    "WARMUP_EPOCHS = 10\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザ\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# 学習率スケジューラ（コサインアニーリング）\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS - WARMUP_EPOCHS\n",
    ")\n",
    "\n",
    "# Warmupスケジューラ\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, base_lr):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        return 1.0\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "warmup_scheduler = warmup_lr_scheduler(optimizer, WARMUP_EPOCHS, LEARNING_RATE)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Warmup epochs: {WARMUP_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 学習・評価関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Evaluate on validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Top-5 accuracy\n",
    "    correct_top5 = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Top-1\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Top-5\n",
    "        _, pred_top5 = outputs.topk(5, 1, True, True)\n",
    "        pred_top5 = pred_top5.t()\n",
    "        correct_top5 += pred_top5.eq(labels.view(1, -1).expand_as(pred_top5)).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'top1': f\"{100.*correct/total:.2f}%\",\n",
    "            'top5': f\"{100.*correct_top5/total:.2f}%\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy_top1 = 100. * correct / total\n",
    "    accuracy_top5 = 100. * correct_top5 / total\n",
    "\n",
    "    return avg_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 学習実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習履歴を記録\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc_top1': [],\n",
    "    'val_acc_top5': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_acc_top1, val_acc_top5 = evaluate(\n",
    "        model, val_loader, criterion, device, epoch\n",
    "    )\n",
    "\n",
    "    # Update learning rate\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc_top1'].append(val_acc_top1)\n",
    "    history['val_acc_top5'].append(val_acc_top5)\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Top-1: {val_acc_top1:.2f}% | Val Top-5: {val_acc_top5:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc_top1 > best_val_acc:\n",
    "        best_val_acc = val_acc_top1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc_top1': val_acc_top1,\n",
    "            'val_acc_top5': val_acc_top5,\n",
    "        }, 'vjepa2_imagenet_best.pth')\n",
    "        print(f\"  ✓ Best model saved (Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}% (Epoch {best_epoch+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Top-1 Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Acc')\n",
    "axes[0, 1].plot(history['val_acc_top1'], label='Val Top-1 Acc')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Top-1 Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Top-5 Accuracy\n",
    "axes[1, 0].plot(history['val_acc_top5'], label='Val Top-5 Acc', color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "axes[1, 0].set_title('Top-5 Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Learning Rate\n",
    "axes[1, 1].plot(history['lr'], color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vjepa2_imagenet_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved to 'vjepa2_imagenet_training.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. モデルの保存とロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終モデルを保存\n",
    "torch.save({\n",
    "    'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'model_size': MODEL_SIZE,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'freeze_encoder': FREEZE_ENCODER,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "    }\n",
    "}, 'vjepa2_imagenet_final.pth')\n",
    "\n",
    "print(\"✓ Final model saved to 'vjepa2_imagenet_final.pth'\")\n",
    "\n",
    "# ベストモデルをロード\n",
    "checkpoint = torch.load('vjepa2_imagenet_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"\\n✓ Best model loaded\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']+1}\")\n",
    "print(f\"  Val Top-1 Acc: {checkpoint['val_acc_top1']:.2f}%\")\n",
    "print(f\"  Val Top-5 Acc: {checkpoint['val_acc_top5']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    Predict class for a single image.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    outputs = model(image_tensor)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "    # Top-5 predictions\n",
    "    top5_prob, top5_idx = torch.topk(probabilities, 5, dim=1)\n",
    "\n",
    "    return top5_idx[0].cpu().numpy(), top5_prob[0].cpu().numpy()\n",
    "\n",
    "\n",
    "# テスト画像で推論\n",
    "test_transform = get_transforms(is_training=False)\n",
    "\n",
    "# サンプル画像を取得（val datasetから）\n",
    "sample_idx = 0\n",
    "sample_image, sample_label = val_dataset[sample_idx]\n",
    "\n",
    "# 推論\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_image_batch = sample_image.unsqueeze(0).to(device)\n",
    "    outputs = model(sample_image_batch)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    top5_prob, top5_idx = torch.topk(probabilities, 5, dim=1)\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(f\"True Label: {sample_label}\")\n",
    "print(\"\\nTop-5 Predictions:\")\n",
    "for i, (idx, prob) in enumerate(zip(top5_idx[0].cpu().numpy(), top5_prob[0].cpu().numpy())):\n",
    "    print(f\"  {i+1}. Class {idx}: {prob*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V-JEPA 2 ImageNet Fine-tuning Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは以下を実装しました：\n",
    "\n",
    "1. **V-JEPA 2の完全実装**\n",
    "   - 3D Position Encoding (sinusoidal)\n",
    "   - 3D Patch Embedding (tubelets)\n",
    "   - Vision Transformer encoder\n",
    "\n",
    "2. **事前学習済みモデルの利用**\n",
    "   - Meta AIの公式重みをロード\n",
    "   - Linear Probing / Full Fine-tuning\n",
    "\n",
    "3. **ImageNetでの事後学習**\n",
    "   - データ拡張\n",
    "   - 学習ループ\n",
    "   - 評価とログ記録\n",
    "\n",
    "### 論文との対応\n",
    "\n",
    "- **Encoder**: ViT-L/H/g (300M-1B parameters)\n",
    "- **Position Encoding**: 3D sinusoidal embeddings\n",
    "- **Patch Size**: 16×16 spatial, 2 temporal (tubelets)\n",
    "- **Training**: Linear probing → Full fine-tuning\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "1. より大きなモデル（ViT-g）でスケールアップ\n",
    "2. より長い学習（100+ epochs）\n",
    "3. 高度なデータ拡張（MixUp, CutMix）\n",
    "4. マルチGPU分散学習\n",
    "\n",
    "### 参考文献\n",
    "\n",
    "- Paper: [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)\n",
    "- Code: [github.com/facebookresearch/vjepa2](https://github.com/facebookresearch/vjepa2)\n",
    "- Blog: [ai.meta.com/vjepa](https://ai.meta.com/vjepa/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
