{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA 2: CIFAR-10 Fine-tuning with Pretrained Weights\n",
    "\n",
    "このノートブックでは、V-JEPA 2の事前学習済みモデルを使用してCIFAR-10でファインチューニングを行う。\n",
    "\n",
    "## 論文情報\n",
    "- **Title**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning\n",
    "- **arXiv**: 2506.09985\n",
    "- **Authors**: Mido Assran et al. (Meta AI)\n",
    "- **Published**: June 2025\n",
    "\n",
    "## 実装内容\n",
    "1. V-JEPA 2モデルの完全実装\n",
    "2. 事前学習済み重みのロード\n",
    "3. CIFAR-10データセットの準備（Google Colab対応）\n",
    "4. メモリ効率的なLinear Probing / Full Fine-tuning\n",
    "5. 学習と評価（バリデーションハング修正済み）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップと依存関係のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境の確認\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# GPU確認\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install -q timm einops\n",
    "!pip install -q torchvision\n",
    "!pip install -q tqdm\n",
    "!pip install -q wandb  # オプション: ログ記録用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import inspect\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. V-JEPA 2モデルの実装\n",
    "\n",
    "論文に忠実な完全実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Position Encoding Utilities\n",
    "# ============================================================================\n",
    "\n",
    "def get_3d_sincos_pos_embed(\n",
    "    embed_dim: int,\n",
    "    grid_size: int,\n",
    "    grid_depth: int,\n",
    "    cls_token: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    3D sinusoidal position embeddings for video (T x H x W).\n",
    "    \n",
    "    Note: Flexibly handles embed_dim not divisible by 3.\n",
    "    Dimensions are partitioned ensuring all are even (required for sinusoidal encoding).\n",
    "    \"\"\"\n",
    "    dim_t = (embed_dim // 3) // 2 * 2\n",
    "    dim_h = ((embed_dim - dim_t) // 2) // 2 * 2\n",
    "    dim_w = embed_dim - dim_t - dim_h\n",
    "\n",
    "    grid_t = np.arange(grid_depth, dtype=np.float32)\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "\n",
    "    grid = np.meshgrid(grid_t, grid_h, grid_w, indexing='ij')\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([3, -1]).T\n",
    "\n",
    "    pos_embed_t = get_1d_sincos_pos_embed_from_grid(dim_t, grid[:, 0])\n",
    "    pos_embed_h = get_1d_sincos_pos_embed_from_grid(dim_h, grid[:, 1])\n",
    "    pos_embed_w = get_1d_sincos_pos_embed_from_grid(dim_w, grid[:, 2])\n",
    "\n",
    "    pos_embed = np.concatenate([pos_embed_t, pos_embed_h, pos_embed_w], axis=1)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return torch.from_numpy(pos_embed).float()\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1D sinusoidal position embeddings.\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "\n",
    "    pos = pos.reshape(-1)\n",
    "    out = np.einsum('m,d->md', pos, omega)\n",
    "\n",
    "    emb_sin = np.sin(out)\n",
    "    emb_cos = np.cos(out)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Patch Embedding\n",
    "# ============================================================================\n",
    "\n",
    "class PatchEmbed3D(nn.Module):\n",
    "    \"\"\"3D patch embedding for video.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        tubelet_size: int = 2,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.tubelet_size = tubelet_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=(tubelet_size, patch_size, patch_size),\n",
    "            stride=(tubelet_size, patch_size, patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int, int]]:\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = self.proj(x)\n",
    "\n",
    "        T_patches = x.shape[2]\n",
    "        H_patches = x.shape[3]\n",
    "        W_patches = x.shape[4]\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x, (T_patches, H_patches, W_patches)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Transformer Components\n",
    "# ============================================================================\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.0\n",
    "            )\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Vision Transformer\n",
    "# ============================================================================\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer for V-JEPA 2.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        tubelet_size: int = 2,\n",
    "        in_channels: int = 3,\n",
    "        num_frames: int = 16,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_scale: Optional[float] = None,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        use_cls_token: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.tubelet_size = tubelet_size\n",
    "        self.num_frames = num_frames\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_cls_token = use_cls_token\n",
    "\n",
    "        self.patch_embed = PatchEmbed3D(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            tubelet_size=tubelet_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        grid_size = img_size // patch_size\n",
    "        grid_depth = num_frames // tubelet_size\n",
    "        num_patches = grid_depth * grid_size * grid_size\n",
    "\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "            num_patches += 1\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        pos_embed = get_3d_sincos_pos_embed(\n",
    "            embed_dim,\n",
    "            grid_size,\n",
    "            grid_depth,\n",
    "            cls_token=use_cls_token\n",
    "        )\n",
    "        self.register_buffer('pos_embed', pos_embed.unsqueeze(0))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        self.apply(self._init_layer_weights)\n",
    "\n",
    "    def _init_layer_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_all_tokens: bool = False) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "\n",
    "        x, _ = self.patch_embed(x)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_all_tokens or self.cls_token is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x[:, 0]\n",
    "\n",
    "\n",
    "print(\"V-JEPA 2 model components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. V-JEPA 2モデルのビルド\n",
    "\n",
    "ViT-L/g構成でモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vjepa2_encoder(model_size='vitl', num_frames=16):\n",
    "    \"\"\"\n",
    "    Build V-JEPA 2 encoder.\n",
    "\n",
    "    Args:\n",
    "        model_size: 'vitl' (300M), 'vith' (600M), or 'vitg' (1B)\n",
    "        num_frames: Number of input frames\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'vitl': {\n",
    "            'embed_dim': 1024,\n",
    "            'depth': 24,\n",
    "            'num_heads': 16,\n",
    "        },\n",
    "        'vith': {\n",
    "            'embed_dim': 1280,\n",
    "            'depth': 32,\n",
    "            'num_heads': 16,\n",
    "        },\n",
    "        'vitg': {\n",
    "            'embed_dim': 1408,\n",
    "            'depth': 40,\n",
    "            'num_heads': 16,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if model_size not in configs:\n",
    "        raise ValueError(f\"Unknown model size: {model_size}\")\n",
    "\n",
    "    config = configs[model_size]\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        tubelet_size=2,\n",
    "        in_channels=3,\n",
    "        num_frames=num_frames,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=4.0,\n",
    "        use_cls_token=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_SIZE = 'vitl'\n",
    "NUM_FRAMES = 4\n",
    "\n",
    "encoder = build_vjepa2_encoder(MODEL_SIZE, NUM_FRAMES)\n",
    "\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: V-JEPA 2 {MODEL_SIZE.upper()}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Number of frames: {NUM_FRAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 事前学習済み重みのダウンロードとロード\n",
    "\n",
    "Meta AIの公式リポジトリから事前学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def load_pretrained_vjepa2(model_size='vitl'):\n",
    "    \"\"\"\n",
    "    Load pretrained V-JEPA 2 weights from official sources.\n",
    "    \n",
    "    Available models:\n",
    "    - vitl: ViT-L/16 (300M params, 256 resolution)\n",
    "    - vith: ViT-H/16 (600M params, 256 resolution)\n",
    "    - vitg: ViT-g/16 (1B params, 256 resolution)\n",
    "    - vitg_384: ViT-g/16 (1B params, 384 resolution)\n",
    "    \"\"\"\n",
    "    \n",
    "    hub_model_mapping = {\n",
    "        'vitl': 'vjepa2_vit_large',\n",
    "        'vith': 'vjepa2_vit_huge',\n",
    "        'vitg': 'vjepa2_vit_giant',\n",
    "        'vitg_384': 'vjepa2_vit_giant_384',\n",
    "    }\n",
    "    \n",
    "    hf_model_mapping = {\n",
    "        'vitl': 'facebook/vjepa2-vitl-fpc64-256',\n",
    "        'vith': 'facebook/vjepa2-vith-fpc64-256',\n",
    "        'vitg': 'facebook/vjepa2-vitg-fpc64-256',\n",
    "        'vitg_384': 'facebook/vjepa2-vitg-fpc64-384',\n",
    "    }\n",
    "    \n",
    "    direct_urls = {\n",
    "        'vitl': 'https://dl.fbaipublicfiles.com/vjepa2/vitl.pt',\n",
    "        'vith': 'https://dl.fbaipublicfiles.com/vjepa2/vith.pt',\n",
    "        'vitg': 'https://dl.fbaipublicfiles.com/vjepa2/vitg.pt',\n",
    "        'vitg_384': 'https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt',\n",
    "    }\n",
    "    \n",
    "    if model_size not in hub_model_mapping:\n",
    "        print(f\"Unknown model size: {model_size}\")\n",
    "        print(f\"Available sizes: {list(hub_model_mapping.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading V-JEPA 2 {model_size.upper()} from PyTorch Hub...\")\n",
    "        hub_model_name = hub_model_mapping[model_size]\n",
    "        \n",
    "        encoder = torch.hub.load(\n",
    "            'facebookresearch/vjepa2',\n",
    "            hub_model_name,\n",
    "            trust_repo=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully loaded {hub_model_name}\")\n",
    "        return encoder\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PyTorch Hub loading failed: {e}\")\n",
    "        print(\"Trying alternative methods...\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading V-JEPA 2 {model_size.upper()} from Hugging Face...\")\n",
    "        from transformers import AutoModel\n",
    "        \n",
    "        hf_model_name = hf_model_mapping[model_size]\n",
    "        encoder = AutoModel.from_pretrained(hf_model_name, trust_remote_code=True)\n",
    "        \n",
    "        print(f\"Successfully loaded {hf_model_name}\")\n",
    "        return encoder\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"transformers not installed. Install with: pip install transformers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Hugging Face loading failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading V-JEPA 2 {model_size.upper()} weights...\")\n",
    "        os.makedirs('./pretrained_weights', exist_ok=True)\n",
    "        \n",
    "        url = direct_urls[model_size]\n",
    "        save_path = f'./pretrained_weights/{model_size}_vjepa2.pt'\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            urllib.request.urlretrieve(url, save_path)\n",
    "            print(f\"Downloaded to {save_path}\")\n",
    "        else:\n",
    "            print(f\"Found cached weights at {save_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(save_path, map_location='cpu')\n",
    "        \n",
    "        print(\"Direct download successful, but requires manual model construction\")\n",
    "        print(\"Please use PyTorch Hub or Hugging Face for automatic loading\")\n",
    "        return checkpoint\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Direct download failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Loading V-JEPA 2 Pretrained Weights\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: V-JEPA 2 {MODEL_SIZE.upper()}\")\n",
    "print(f\"Parameters: ~{['300M', '600M', '1B', '1B'][['vitl', 'vith', 'vitg', 'vitg_384'].index(MODEL_SIZE)]}\")\n",
    "print(f\"Frames: {NUM_FRAMES}\\n\")\n",
    "\n",
    "pretrained_result = load_pretrained_vjepa2(MODEL_SIZE)\n",
    "\n",
    "if pretrained_result is None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WARNING: Could not load pretrained weights\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Proceeding with random initialization\")\n",
    "    print(\"Note: Results will be significantly worse without pretrained weights\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Install transformers: pip install transformers\")\n",
    "    print(\"3. Try running: git clone https://github.com/facebookresearch/vjepa2\")\n",
    "    print(\"=\"*70)\n",
    "    pretrained_encoder = encoder\n",
    "elif isinstance(pretrained_result, tuple):\n",
    "    print(\"\\nReceived tuple from pretrained loader, extracting model...\")\n",
    "    pretrained_encoder = pretrained_result[0] if len(pretrained_result) > 0 else encoder\n",
    "elif isinstance(pretrained_result, dict):\n",
    "    print(\"\\nReceived checkpoint dict, using encoder with random weights\")\n",
    "    print(\"(Manual state_dict loading required - not implemented)\")\n",
    "    pretrained_encoder = encoder\n",
    "elif hasattr(pretrained_result, 'parameters'):\n",
    "    pretrained_encoder = pretrained_result\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUCCESS: Using official V-JEPA 2 pretrained weights\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {MODEL_SIZE.upper()}\")\n",
    "    print(f\"Source: Meta AI / FAIR\")\n",
    "    print(f\"Pretrained on: Video datasets\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(f\"\\nUnexpected type from pretrained loader: {type(pretrained_result)}\")\n",
    "    print(\"Using encoder with random weights\")\n",
    "    pretrained_encoder = encoder\n",
    "\n",
    "encoder = pretrained_encoder\n",
    "\n",
    "print(\"\\nEncoder ready for CIFAR-10 fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ImageNet分類ヘッドの追加\n",
    "\n",
    "事前学習済みエンコーダに分類ヘッドを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    V-JEPA 2 encoder + linear classification head for image classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        num_classes: int = 10,\n",
    "        freeze_encoder: bool = True,\n",
    "        use_video_frames: bool = False,\n",
    "        num_frames: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.use_video_frames = use_video_frames\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        forward_sig = inspect.signature(encoder.forward)\n",
    "        self.supports_return_all_tokens = 'return_all_tokens' in forward_sig.parameters\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Encoder frozen (Linear Probing mode)\")\n",
    "        else:\n",
    "            print(\"Encoder unfrozen (Full Fine-tuning mode)\")\n",
    "\n",
    "        embed_dim = encoder.embed_dim\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 4:\n",
    "            x = x.unsqueeze(2).repeat(1, 1, self.num_frames, 1, 1)\n",
    "\n",
    "        if self.supports_return_all_tokens:\n",
    "            features = self.encoder(x, return_all_tokens=False)\n",
    "        else:\n",
    "            features = self.encoder(x)\n",
    "            if features.dim() == 3:\n",
    "                features = features[:, 0]\n",
    "\n",
    "        logits = self.head(features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "FREEZE_ENCODER = True\n",
    "\n",
    "model = ImageNetClassifier(\n",
    "    encoder=pretrained_encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_encoder=FREEZE_ENCODER,\n",
    "    num_frames=NUM_FRAMES\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Target classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CIFAR-10データセットの準備\n",
    "\n",
    "CIFAR-10データセットをロードして前処理（Google Colab最適化済み）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "def get_transforms(is_training=True):\n",
    "    \"\"\"\n",
    "    Get CIFAR-10 transforms.\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(224),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "train_dataset = CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=get_transforms(is_training=True)\n",
    ")\n",
    "val_dataset = CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=get_transforms(is_training=False)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"CIFAR-10 dataset loaded\")\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001 if FREEZE_ENCODER else 0.0001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS - WARMUP_EPOCHS\n",
    ")\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, base_lr):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        return 1.0\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "warmup_scheduler = warmup_lr_scheduler(optimizer, WARMUP_EPOCHS, LEARNING_RATE)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup epochs: {WARMUP_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Dataset: CIFAR-10\")\n",
    "print(f\"Classes: 10\")\n",
    "print(f\"Training mode: {'Linear Probing' if FREEZE_ENCODER else 'Full Fine-tuning'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 学習・評価関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch with memory optimization.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{100.*correct/total:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Evaluate on validation set with memory optimization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_top5 = 0\n",
    "    compute_top5 = len(val_loader.dataset.classes) >= 10 if hasattr(val_loader.dataset, 'classes') else False\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.detach().cpu().item()\n",
    "\n",
    "        _, predicted = outputs.detach().max(1)\n",
    "        batch_correct = predicted.eq(labels).sum().item()\n",
    "        correct += batch_correct\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if compute_top5:\n",
    "            _, pred_top5 = outputs.detach().topk(5, 1, True, True)\n",
    "            pred_top5 = pred_top5.t()\n",
    "            correct_top5 += pred_top5.eq(labels.view(1, -1).expand_as(pred_top5)).sum().item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            postfix = {\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{100.*correct/total:.2f}%\"\n",
    "            }\n",
    "            if compute_top5:\n",
    "                postfix['top5'] = f\"{100.*correct_top5/total:.2f}%\"\n",
    "            pbar.set_postfix(postfix)\n",
    "        \n",
    "        if batch_idx % 20 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy_top1 = 100. * correct / total\n",
    "    accuracy_top5 = 100. * correct_top5 / total if compute_top5 else 0.0\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 学習実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc_top1': [],\n",
    "    'val_acc_top5': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc_top1, val_acc_top5 = evaluate(\n",
    "        model, val_loader, criterion, device, epoch\n",
    "    )\n",
    "\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc_top1'].append(val_acc_top1)\n",
    "    history['val_acc_top5'].append(val_acc_top5)\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Top-1: {val_acc_top1:.2f}%\")\n",
    "    if val_acc_top5 > 0:\n",
    "        print(f\"  Val Top-5: {val_acc_top5:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "    if val_acc_top1 > best_val_acc:\n",
    "        best_val_acc = val_acc_top1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc_top1': val_acc_top1,\n",
    "            'val_acc_top5': val_acc_top5,\n",
    "        }, 'vjepa2_cifar10_best.pth')\n",
    "        print(f\"  Best model saved (Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"  Memory cleanup performed\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}% (Epoch {best_epoch+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Acc')\n",
    "axes[0, 1].plot(history['val_acc_top1'], label='Val Acc')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "if any(acc > 0 for acc in history['val_acc_top5']):\n",
    "    axes[1, 0].plot(history['val_acc_top5'], label='Val Top-5 Acc', color='green')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].set_title('Top-5 Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Top-5 not computed\\n(10 classes only)', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Top-5 Accuracy')\n",
    "\n",
    "axes[1, 1].plot(history['lr'], color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vjepa2_cifar10_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to 'vjepa2_cifar10_training.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. モデルの保存とロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'model_size': MODEL_SIZE,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'freeze_encoder': FREEZE_ENCODER,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'dataset': 'CIFAR-10'\n",
    "    }\n",
    "}, 'vjepa2_cifar10_final.pth')\n",
    "\n",
    "print(\"Final model saved to 'vjepa2_cifar10_final.pth'\")\n",
    "\n",
    "checkpoint = torch.load('vjepa2_cifar10_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"\\nBest model loaded\")\n",
    "print(f\"Epoch: {checkpoint['epoch']+1}\")\n",
    "print(f\"Val Acc: {checkpoint['val_acc_top1']:.2f}%\")\n",
    "if checkpoint['val_acc_top5'] > 0:\n",
    "    print(f\"Val Top-5 Acc: {checkpoint['val_acc_top5']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    Predict class for a single image.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    outputs = model(image_tensor)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "    top5_prob, top5_idx = torch.topk(probabilities, min(5, NUM_CLASSES), dim=1)\n",
    "\n",
    "    return top5_idx[0].cpu().numpy(), top5_prob[0].cpu().numpy()\n",
    "\n",
    "\n",
    "test_transform = get_transforms(is_training=False)\n",
    "\n",
    "sample_idx = 0\n",
    "sample_image, sample_label = val_dataset[sample_idx]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_image_batch = sample_image.unsqueeze(0).to(device)\n",
    "    outputs = model(sample_image_batch)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    top5_prob, top5_idx = torch.topk(probabilities, min(5, NUM_CLASSES), dim=1)\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(f\"True Label: {sample_label} ({CIFAR10_CLASSES[sample_label]})\")\n",
    "print(f\"\\nTop-{min(5, NUM_CLASSES)} Predictions:\")\n",
    "for i, (idx, prob) in enumerate(zip(top5_idx[0].cpu().numpy(), top5_prob[0].cpu().numpy())):\n",
    "    print(f\"  {i+1}. Class {idx} ({CIFAR10_CLASSES[idx]}): {prob*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V-JEPA 2 CIFAR-10 Fine-tuning Complete\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
